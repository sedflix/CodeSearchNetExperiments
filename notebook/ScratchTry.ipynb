{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../codesearchnet\")\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.datasets import Planetoid\n",
    "# import torch_geometric.transforms as T\n",
    "# from torch_geometric.nn import GCNConv, GAE, VGAE\n",
    "# from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "\n",
    "import swifter\n",
    "import fasttext as ft\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from livelossplot import PlotLosses\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda, LSTM, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "\n",
    "from code_parser import *\n",
    "from data_reader import get_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim_c, embeddings_dim_q = 128, 128\n",
    "max_len_code, max_len_query = 128, 64\n",
    "vocab_size_code = 100000\n",
    "vocab_size_query = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(split):\n",
    "    df = get_data_df(\"../resources/data/\", [\"python\"] , [split])\n",
    "    \n",
    "    array_querys = np.array(df['docstring_tokens'].apply(lambda x : \" \".join(x)))\n",
    "    array_codes = np.array(df['code_tokens'].apply(lambda x : \" \".join(x)))\n",
    "    \n",
    "    query_dataset = tf.data.Dataset.from_tensor_slices(array_querys)\n",
    "    code_dataset = tf.data.Dataset.from_tensor_slices(array_codes)\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((query_dataset, code_dataset)).batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(\"train\")\n",
    "valid_ds = get_dataset(\"valid\")\n",
    "test_ds = get_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_code = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=vocab_size_code,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_len_code,\n",
    ")\n",
    "\n",
    "vectorize_query = TextVectorization(\n",
    "    max_tokens=vocab_size_query,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_len_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_code.adapt(train_ds.map(lambda q,c: c))\n",
    "vectorize_query.adapt(train_ds.map(lambda q,c: q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(query, code):\n",
    "    query = tf.expand_dims(query, -1)\n",
    "    code = tf.expand_dims(code, -1)\n",
    "    return vectorize_query(query), vectorize_code(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(vectorize)\n",
    "valid_ds = valid_ds.map(vectorize)\n",
    "test_ds = test_ds.map(vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.prefetch(buffer_size=10)\n",
    "valid_ds = valid_ds.prefetch(buffer_size=10)\n",
    "test_ds = test_ds.prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "mrr_tracker = tf.keras.metrics.Mean(name=\"mrr\")\n",
    "\n",
    "class CustomModel(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(data, training=True)\n",
    "            loss_value = loss_(None, logits)\n",
    "        \n",
    "        mrr_value = mrr(None, logits)\n",
    "\n",
    "        gradients = tape.gradient(loss_value, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss_value)\n",
    "        mrr_tracker.update_state(mrr_value)\n",
    "        return {\"loss\": loss_tracker.result(), \"mrr\": mrr_tracker.result()}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "            \n",
    "        logits = self(data, training=False)\n",
    "        loss_value = loss_(None, logits)\n",
    "        mrr_value = mrr(None, logits)\n",
    "        \n",
    "        \n",
    "        loss_tracker.update_state(loss_value)\n",
    "        mrr_tracker.update_state(mrr_value)\n",
    "        return {\"loss\": loss_tracker.result(), \"mrr\": mrr_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "input_q = Embedding(len(vectorize_query.get_vocabulary()) + 1, embeddings_dim_q)(input_query)\n",
    "\n",
    "input_code = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "input_c = Embedding(len(vectorize_code.get_vocabulary()) + 1, embeddings_dim_c)(input_code)\n",
    "\n",
    "lstm_q = LSTM(embeddings_dim_q, dropout=0.3, recurrent_dropout=0.0)(input_q)\n",
    "lstm_c = LSTM(embeddings_dim_c, dropout=0.3, recurrent_dropout=0.0)(input_c)\n",
    "\n",
    "\n",
    "model = CustomModel([input_query, input_code], [lstm_q, lstm_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_(y_true, y_pred):\n",
    "    \n",
    "    q, c = y_pred\n",
    "\n",
    "    similarity_score = tf.matmul(q, K.transpose(c))\n",
    "    per_sample_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=similarity_score,\n",
    "        labels=tf.range(q.shape[0])\n",
    "    )\n",
    "    return tf.reduce_sum(per_sample_loss) / q.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mrr(y_true, y_pred):\n",
    "    q,c = y_pred\n",
    "    \n",
    "    similarity_scores = tf.matmul(q, K.transpose(c))\n",
    "    \n",
    "    # extract the logits from the diagonal of the matrix, which are the logits corresponding to the ground-truth\n",
    "    correct_scores = tf.linalg.diag_part(similarity_scores)\n",
    "    \n",
    "    # compute how many queries have bigger logits than the ground truth (the diagonal) -> which will be incorrectly ranked\n",
    "    compared_scores = similarity_scores >= tf.expand_dims(correct_scores, axis=-1)\n",
    "    \n",
    "    compared_scores = tf.cast(compared_scores, tf.float16)\n",
    "    # for each row of the matrix (query), sum how many logits are larger than the ground truth\n",
    "    # ...then take the reciprocal of that to get the MRR for each individual query (you will need to take the mean later)\n",
    "    return K.mean(tf.math.reciprocal(tf.reduce_sum(compared_scores, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"./exp/vectorise_model.h5\", monitor='val_mrr', verbose=0, save_best_only=True, mode='max', save_freq='epoch')\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir='./exp/vectorise_model1', update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.unbatch().batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, validation_data=valid_ds, epochs=10, callbacks=[checkpoint, tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/10\n",
    "  15/3221 [..............................] - ETA: 2:07:26 - loss: 4.8522 - mrr: 0.0409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig using loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(100)\n",
    "valid_ds = valid_ds.cache().prefetch(100)\n",
    "test_ds = valid_ds.cache().prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "logs = {}\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # TRAINING\n",
    "    losses = []\n",
    "    mrrs = []\n",
    "    for x in train_ds:\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            loss_value = loss_(None, logits)\n",
    "\n",
    "        mrr_value = mrr(None, logits)\n",
    "\n",
    "        # calculate gradient\n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "        losses.append(loss_value)\n",
    "        mrrs.append(mrr_value)\n",
    "\n",
    "        print(f\"Epoch: {epoch}; Loss: {loss_value}; MRR: {mrr_value} <- Train\", end=\"\\r\")\n",
    "        \n",
    "    logs['loss'] = np.mean(losses)\n",
    "    logs['mrr'] = np.mean(mrrs)\n",
    "    \n",
    "    \n",
    "    # VALIDATION\n",
    "    losses = []\n",
    "    mrrs = []\n",
    "    for x in valid_ds:\n",
    "        \n",
    "        logits = model(x)\n",
    "        \n",
    "        loss_value = loss_(None, logits)\n",
    "        mrr_value = mrr(None, logits)\n",
    "        \n",
    "        losses.append(loss_value)\n",
    "        mrrs.append(mrr_value)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}; Loss: {loss_value}; MRR: {mrr_value} <- Test\", end=\"\\r\")\n",
    "        \n",
    "    logs['val_loss'] = np.mean(losses)\n",
    "    logs['val_mrr'] = np.mean(mrrs)\n",
    "    \n",
    "    liveloss.update(logs)\n",
    "    liveloss.send()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch: 0; Loss: 4.852148056030273; MRR: 0.0242462158203125 <- Trai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim_c, embeddings_dim_q = 256, 256\n",
    "max_len_code, max_len_query = 64, 32\n",
    "vocab_size_code = 500000\n",
    "vocab_size_query = 250000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _code_parse(row):\n",
    "#     parser_ = get_parser(lang=row['language'], so_path=\"../resources/csnet_parse_build.so\")\n",
    "#     g_ = parse_program(row['code'], parser=parser_)\n",
    "#     return g_\n",
    "\n",
    "# model = ft.train_unsupervised(\n",
    "#     \"../resources/python_processed/query_corpus.txt\",\n",
    "#     lr = 0.005,\n",
    "#     epoch = 30,\n",
    "#     dim = 256,\n",
    "#     thread = 40,\n",
    "# )\n",
    "# model.save_model(\"../resources/python_processed/query_ft.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ft = ft.load_model(\"../resources/python_processed/query_ft.bin\")\n",
    "code_ft = ft.load_model(\"../resources/python_processed/code_ft.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(split):\n",
    "    df = get_data_df(\"../resources/data/\", [\"python\"] , [split])\n",
    "    \n",
    "    q_s, c_s = [], []\n",
    "    for i, row in df[['docstring_tokens','code_tokens']].iterrows():\n",
    "        _query = [query_ft[token] for token in row[0]]\n",
    "        _code = [code_ft[token] for token in row[1]]\n",
    "\n",
    "        q_s.append(_query)\n",
    "        c_s.append(_code)\n",
    "    \n",
    "    q_s = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        q_s, maxlen=max_len_query, dtype='float', padding='post', truncating='post',\n",
    "        value=[0]\n",
    "    )\n",
    "\n",
    "    c_s = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        c_s, maxlen=max_len_code, dtype='float', padding='post', truncating='post',\n",
    "        value=[0]\n",
    "    )\n",
    "    \n",
    "    query_dataset = tf.data.Dataset.from_tensor_slices(q_s)\n",
    "    code_dataset = tf.data.Dataset.from_tensor_slices(c_s)\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((query_dataset, code_dataset))\n",
    "    \n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(\"train\")\n",
    "valid_ds = get_dataset(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.unbatch().batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_(y_true, y_pred):\n",
    "    \n",
    "    q, c = y_pred\n",
    "\n",
    "    similarity_score = tf.matmul(q, K.transpose(c))\n",
    "    per_sample_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=similarity_score,\n",
    "        labels=tf.range(q.shape[0])\n",
    "    )\n",
    "    return tf.reduce_sum(per_sample_loss) / q.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "def mrr(y_true, y_pred):\n",
    "    q,c = y_pred\n",
    "    \n",
    "    similarity_scores = tf.matmul(q, K.transpose(c))\n",
    "    \n",
    "    # extract the logits from the diagonal of the matrix, which are the logits corresponding to the ground-truth\n",
    "    correct_scores = tf.linalg.diag_part(similarity_scores)\n",
    "    \n",
    "    # compute how many queries have bigger logits than the ground truth (the diagonal) -> which will be incorrectly ranked\n",
    "    compared_scores = similarity_scores >= tf.expand_dims(correct_scores, axis=-1)\n",
    "    \n",
    "    compared_scores = tf.cast(compared_scores, tf.float16)\n",
    "    # for each row of the matrix (query), sum how many logits are larger than the ground truth\n",
    "    # ...then take the reciprocal of that to get the MRR for each individual query (you will need to take the mean later)\n",
    "    return K.mean(tf.math.reciprocal(tf.reduce_sum(compared_scores, axis=1)))\n",
    "\n",
    "\n",
    "input_query = tf.keras.Input(shape=(max_len_query, embeddings_dim_q))\n",
    "input_code = tf.keras.Input(shape=(max_len_code, embeddings_dim_c))\n",
    "\n",
    "lstm_q = LSTM(embeddings_dim_q, dropout=0.3, recurrent_dropout=0.0)(input_query)\n",
    "lstm_c = LSTM(embeddings_dim_c, dropout=0.3, recurrent_dropout=0.0)(input_code)\n",
    "\n",
    "model = tf.keras.Model([input_query, input_code], [lstm_q, lstm_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "logs = {}\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # TRAINING\n",
    "    losses = []\n",
    "    mrrs = []\n",
    "    for x in train_ds:\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            loss_value = loss_(None, logits)\n",
    "\n",
    "        mrr_value = mrr(None, logits)\n",
    "\n",
    "        # calculate gradient\n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "        losses.append(loss_value)\n",
    "        mrrs.append(mrr_value)\n",
    "\n",
    "        print(f\"Epoch: {epoch}; Loss: {loss_value}; MRR: {mrr_value} <- Train\", end=\"\\r\")\n",
    "        \n",
    "    logs['loss'] = np.mean(losses)\n",
    "    logs['mrr'] = np.mean(mrrs)\n",
    "    \n",
    "    \n",
    "    # VALIDATION\n",
    "    losses = []\n",
    "    mrrs = []\n",
    "    for x in valid_ds:\n",
    "        \n",
    "        logits = model(x)\n",
    "        \n",
    "        loss_value = loss_(None, logits)\n",
    "        mrr_value = mrr(None, logits)\n",
    "        \n",
    "        losses.append(loss_value)\n",
    "        mrrs.append(mrr_value)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}; Loss: {loss_value}; MRR: {mrr_value} <- Test\", end=\"\\r\")\n",
    "        \n",
    "    logs['val_loss'] = np.mean(losses)\n",
    "    logs['val_mrr'] = np.mean(mrrs)\n",
    "    \n",
    "    liveloss.update(logs)\n",
    "    liveloss.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    losses = []\n",
    "    mrrs = []\n",
    "    for x in get_dataset(\"test\"):\n",
    "        \n",
    "        logits = model(x)\n",
    "        \n",
    "        loss_value = loss_(None, logits)\n",
    "        mrr_value = mrr(None, logits)\n",
    "        \n",
    "        losses.append(loss_value)\n",
    "        mrrs.append(mrr_value)\n",
    "        \n",
    "        print(f\"Epoch: test; Loss: {loss_value}; MRR: {mrr_value} <- Test\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mrrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"exp/sratch_keras_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predit/Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"exp/sratch_keras_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder = tf.keras.models.Model(model.get_layer('input_5').input, model.get_layer('lstm_4').output)\n",
    "code_encoder = tf.keras.models.Model(model.get_layer('input_6').input, model.get_layer('lstm_5').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querys = []\n",
    "codes = []\n",
    "\n",
    "for q,c in test_ds.take(100):\n",
    "    querys.extend(query_encoder(q))\n",
    "    codes.extend(code_encoder(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import random\n",
    "\n",
    "t = AnnoyIndex(256, 'angular')\n",
    "for i in range(len(codes)):\n",
    "    t.add_item(i, codes[i])\n",
    "t.build(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = get_data_df(\"../resources/data/\", [\"python\"] , [\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = t.get_nns_by_vector(codes[2155], n=10, include_distances=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0 \n",
    "for i in range(len(querys)):\n",
    "    result = t.get_nns_by_vector(querys[i], n=5, include_distances=False)\n",
    "    if i in result:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/len(querys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../resources/data/python_dedupe_definitions_v2.pkl\", \"rb\") as f:\n",
    "    import pickle \n",
    "    definations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
